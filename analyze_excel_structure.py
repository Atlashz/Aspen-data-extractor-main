#!/usr/bin/env python3
"""
Excelç»“æ„æ·±åº¦åˆ†æå·¥å…·
ä¸“é—¨ç”¨äºåˆ†æBFG-CO2H-HEX.xlsxæ–‡ä»¶ç»“æ„ï¼Œè¯†åˆ«æ•°æ®æå–é—®é¢˜
"""

import pandas as pd
import os
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import numpy as np

class ExcelStructureAnalyzer:
    """Excelæ–‡ä»¶ç»“æ„æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self, excel_file: str):
        self.excel_file = excel_file
        self.analysis_results = {}
        self.all_worksheets = {}
        
    def analyze_complete_structure(self) -> Dict[str, Any]:
        """å®Œæ•´åˆ†æExcelæ–‡ä»¶ç»“æ„"""
        print(f"ğŸ” å¼€å§‹åˆ†æExcelæ–‡ä»¶: {self.excel_file}")
        print("=" * 80)
        
        if not os.path.exists(self.excel_file):
            return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {self.excel_file}"}
        
        results = {
            "file_info": self._get_file_info(),
            "worksheets": {},
            "summary": {},
            "data_patterns": {},
            "column_analysis": {},
            "recommendations": []
        }
        
        try:
            # 1. è·å–æ‰€æœ‰å·¥ä½œè¡¨ä¿¡æ¯
            print("ğŸ“‹ åˆ†æå·¥ä½œè¡¨ç»“æ„...")
            worksheets_info = self._analyze_all_worksheets()
            results["worksheets"] = worksheets_info
            
            # 2. åˆ†ææ•°æ®æ¨¡å¼
            print("ğŸ“Š åˆ†ææ•°æ®æ¨¡å¼...")
            data_patterns = self._analyze_data_patterns()
            results["data_patterns"] = data_patterns
            
            # 3. åˆ†æåˆ—åç»“æ„
            print("ğŸ·ï¸ åˆ†æåˆ—åç»“æ„...")
            column_analysis = self._analyze_column_structures()
            results["column_analysis"] = column_analysis
            
            # 4. ç”Ÿæˆæ‘˜è¦
            print("ğŸ“ˆ ç”Ÿæˆåˆ†ææ‘˜è¦...")
            summary = self._generate_summary()
            results["summary"] = summary
            
            # 5. ç”Ÿæˆä¿®å¤å»ºè®®
            print("ğŸ’¡ ç”Ÿæˆä¿®å¤å»ºè®®...")
            recommendations = self._generate_recommendations()
            results["recommendations"] = recommendations
            
        except Exception as e:
            results["error"] = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        
        self.analysis_results = results
        return results
    
    def _get_file_info(self) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
        stat = os.stat(self.excel_file)
        return {
            "file_path": self.excel_file,
            "file_size_bytes": stat.st_size,
            "file_size_mb": round(stat.st_size / 1024 / 1024, 2),
            "last_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "analysis_time": datetime.now().isoformat()
        }
    
    def _analyze_all_worksheets(self) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰å·¥ä½œè¡¨"""
        worksheets_info = {}
        
        try:
            # è·å–æ‰€æœ‰å·¥ä½œè¡¨åç§°
            xl_file = pd.ExcelFile(self.excel_file, engine='openpyxl')
            sheet_names = xl_file.sheet_names
            
            print(f"   å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œè¡¨: {sheet_names}")
            
            for sheet_name in sheet_names:
                print(f"   åˆ†æå·¥ä½œè¡¨: {sheet_name}")
                
                try:
                    # è¯»å–å·¥ä½œè¡¨æ•°æ®
                    df = pd.read_excel(self.excel_file, sheet_name=sheet_name, engine='openpyxl')
                    self.all_worksheets[sheet_name] = df
                    
                    # åˆ†æå·¥ä½œè¡¨ç»“æ„
                    sheet_info = {
                        "shape": df.shape,
                        "columns": list(df.columns),
                        "column_count": len(df.columns),
                        "row_count": len(df),
                        "data_types": df.dtypes.astype(str).to_dict(),
                        "null_counts": df.isnull().sum().to_dict(),
                        "non_null_counts": df.count().to_dict(),
                        "sample_data": {},
                        "numeric_columns": [],
                        "text_columns": [],
                        "mixed_columns": []
                    }
                    
                    # åˆ†æåˆ—ç±»å‹
                    for col in df.columns:
                        col_data = df[col].dropna()
                        if len(col_data) > 0:
                            if pd.api.types.is_numeric_dtype(col_data):
                                sheet_info["numeric_columns"].append(col)
                            elif pd.api.types.is_string_dtype(col_data) or col_data.dtype == 'object':
                                # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆç±»å‹
                                numeric_count = sum(pd.to_numeric(col_data, errors='coerce').notna())
                                if numeric_count > 0 and numeric_count < len(col_data):
                                    sheet_info["mixed_columns"].append(col)
                                else:
                                    sheet_info["text_columns"].append(col)
                    
                    # è·å–æ ·æœ¬æ•°æ®
                    if not df.empty:
                        sheet_info["sample_data"] = {
                            "first_row": df.iloc[0].to_dict() if len(df) > 0 else {},
                            "last_row": df.iloc[-1].to_dict() if len(df) > 0 else {},
                            "sample_rows": df.head(3).to_dict('records') if len(df) > 0 else []
                        }
                    
                    worksheets_info[sheet_name] = sheet_info
                    print(f"     âœ… {sheet_name}: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
                    
                except Exception as e:
                    worksheets_info[sheet_name] = {"error": f"è¯»å–å¤±è´¥: {str(e)}"}
                    print(f"     âŒ {sheet_name}: è¯»å–å¤±è´¥ - {str(e)}")
            
        except Exception as e:
            return {"error": f"æ— æ³•æ‰“å¼€Excelæ–‡ä»¶: {str(e)}"}
        
        return worksheets_info
    
    def _analyze_data_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®æ¨¡å¼"""
        patterns = {
            "heat_exchanger_indicators": [],
            "temperature_patterns": [],
            "pressure_patterns": [],
            "flow_patterns": [],
            "duty_patterns": [],
            "area_patterns": [],
            "stream_patterns": [],
            "potential_hex_data": {}
        }
        
        # å®šä¹‰æœç´¢æ¨¡å¼
        search_patterns = {
            "heat_exchanger_indicators": [
                'heat', 'exchanger', 'hex', 'hx', 'æ¢çƒ­', 'æ¢çƒ­å™¨', 'cooler', 'heater', 
                'condenser', 'reboiler', 'boiler', 'å†·å´å™¨', 'åŠ çƒ­å™¨', 'å†·å‡å™¨', 'å†æ²¸å™¨'
            ],
            "temperature_patterns": [
                'temp', 'temperature', 'æ¸©åº¦', 'inlet', 'outlet', 'in', 'out', 
                'hot', 'cold', 'çƒ­', 'å†·', 'è¿›å£', 'å‡ºå£', 'å…¥å£', 'shell', 'tube'
            ],
            "pressure_patterns": [
                'press', 'pressure', 'å‹åŠ›', 'å‹å¼º', 'bar', 'psi', 'pa', 'mpa', 'kpa'
            ],
            "flow_patterns": [
                'flow', 'mass', 'volume', 'molar', 'æµé‡', 'è´¨é‡', 'ä½“ç§¯', 'æ‘©å°”', 
                'kg/h', 'kmol/h', 'm3/h', 'rate', 'é€Ÿç‡'
            ],
            "duty_patterns": [
                'duty', 'load', 'heat', 'power', 'è´Ÿè·', 'çƒ­è´Ÿè·', 'åŠŸç‡', 'kw', 'mw', 
                'kj/h', 'mj/h', 'btu', 'kcal'
            ],
            "area_patterns": [
                'area', 'surface', 'é¢ç§¯', 'æ¢çƒ­é¢ç§¯', 'm2', 'mÂ²', 'ft2', 'ftÂ²'
            ],
            "stream_patterns": [
                'stream', 'side', 'shell', 'tube', 'æµè‚¡', 'å£³ç¨‹', 'ç®¡ç¨‹', 
                'hot side', 'cold side', 'process', 'utility'
            ]
        }
        
        # æœç´¢æ‰€æœ‰å·¥ä½œè¡¨ä¸­çš„æ¨¡å¼
        for sheet_name, df in self.all_worksheets.items():
            if isinstance(df, pd.DataFrame):
                sheet_patterns = {}
                
                for pattern_type, keywords in search_patterns.items():
                    matching_columns = []
                    for col in df.columns:
                        col_str = str(col).lower()
                        for keyword in keywords:
                            if keyword.lower() in col_str:
                                matching_columns.append({
                                    "column": col,
                                    "keyword": keyword,
                                    "match_position": col_str.find(keyword.lower())
                                })
                                break
                    
                    if matching_columns:
                        patterns[pattern_type].extend(matching_columns)
                        sheet_patterns[pattern_type] = matching_columns
                
                # è¯„ä¼°è¯¥å·¥ä½œè¡¨æ˜¯å¦åŒ…å«çƒ­äº¤æ¢å™¨æ•°æ®
                hex_score = 0
                hex_indicators = []
                
                if sheet_patterns.get("heat_exchanger_indicators"):
                    hex_score += 3
                    hex_indicators.append("åŒ…å«çƒ­äº¤æ¢å™¨ç›¸å…³åˆ—å")
                
                if sheet_patterns.get("temperature_patterns"):
                    hex_score += 2
                    hex_indicators.append("åŒ…å«æ¸©åº¦ç›¸å…³åˆ—å")
                
                if sheet_patterns.get("duty_patterns"):
                    hex_score += 2
                    hex_indicators.append("åŒ…å«çƒ­è´Ÿè·ç›¸å…³åˆ—å")
                
                if sheet_patterns.get("area_patterns"):
                    hex_score += 2
                    hex_indicators.append("åŒ…å«é¢ç§¯ç›¸å…³åˆ—å")
                
                if sheet_patterns.get("stream_patterns"):
                    hex_score += 1
                    hex_indicators.append("åŒ…å«æµè‚¡ç›¸å…³åˆ—å")
                
                patterns["potential_hex_data"][sheet_name] = {
                    "hex_score": hex_score,
                    "indicators": hex_indicators,
                    "patterns": sheet_patterns,
                    "likely_hex_sheet": hex_score >= 3
                }
        
        return patterns
    
    def _analyze_column_structures(self) -> Dict[str, Any]:
        """åˆ†æåˆ—åç»“æ„"""
        column_analysis = {
            "all_columns": [],
            "column_patterns": {},
            "naming_conventions": {},
            "suggested_mappings": {}
        }
        
        all_columns = []
        for sheet_name, df in self.all_worksheets.items():
            if isinstance(df, pd.DataFrame):
                for col in df.columns:
                    all_columns.append({
                        "sheet": sheet_name,
                        "column": str(col),
                        "column_lower": str(col).lower(),
                        "length": len(str(col)),
                        "has_chinese": any('\u4e00' <= char <= '\u9fff' for char in str(col)),
                        "has_underscore": '_' in str(col),
                        "has_space": ' ' in str(col),
                        "has_number": any(char.isdigit() for char in str(col))
                    })
        
        column_analysis["all_columns"] = all_columns
        
        # åˆ†æå‘½åçº¦å®š
        naming_conventions = {
            "chinese_columns": len([c for c in all_columns if c["has_chinese"]]),
            "underscore_columns": len([c for c in all_columns if c["has_underscore"]]),
            "space_columns": len([c for c in all_columns if c["has_space"]]),
            "number_columns": len([c for c in all_columns if c["has_number"]]),
            "total_columns": len(all_columns),
            "unique_columns": len(set(c["column_lower"] for c in all_columns))
        }
        
        column_analysis["naming_conventions"] = naming_conventions
        
        # ç”Ÿæˆå»ºè®®çš„åˆ—åæ˜ å°„
        suggested_mappings = self._generate_column_mappings(all_columns)
        column_analysis["suggested_mappings"] = suggested_mappings
        
        return column_analysis
    
    def _generate_column_mappings(self, all_columns: List[Dict]) -> Dict[str, List[str]]:
        """ç”Ÿæˆå»ºè®®çš„åˆ—åæ˜ å°„"""
        mappings = {
            "equipment_name": [],
            "duty": [],
            "area": [],
            "hot_stream_name": [],
            "cold_stream_name": [],
            "hot_inlet_temp": [],
            "hot_outlet_temp": [],
            "cold_inlet_temp": [],
            "cold_outlet_temp": [],
            "hot_flow": [],
            "cold_flow": [],
            "pressure": []
        }
        
        # å®šä¹‰æ˜ å°„è§„åˆ™
        mapping_rules = {
            "equipment_name": ['name', 'id', 'tag', 'equipment', 'è®¾å¤‡', 'åç§°', 'hex'],
            "duty": ['duty', 'load', 'heat', 'è´Ÿè·', 'çƒ­è´Ÿè·', 'kw', 'mw'],
            "area": ['area', 'é¢ç§¯', 'm2', 'mÂ²', 'surface'],
            "hot_stream_name": ['hot', 'shell', 'çƒ­', 'å£³ç¨‹', 'hot stream', 'hot side'],
            "cold_stream_name": ['cold', 'tube', 'å†·', 'ç®¡ç¨‹', 'cold stream', 'cold side'],
            "hot_inlet_temp": ['hot', 'inlet', 'in', 'è¿›å£', 'å…¥å£', 'shell', 'temp'],
            "hot_outlet_temp": ['hot', 'outlet', 'out', 'å‡ºå£', 'shell', 'temp'],
            "cold_inlet_temp": ['cold', 'inlet', 'in', 'è¿›å£', 'å…¥å£', 'tube', 'temp'],
            "cold_outlet_temp": ['cold', 'outlet', 'out', 'å‡ºå£', 'tube', 'temp'],
            "hot_flow": ['hot', 'flow', 'mass', 'æµé‡', 'shell', 'kg/h'],
            "cold_flow": ['cold', 'flow', 'mass', 'æµé‡', 'tube', 'kg/h'],
            "pressure": ['press', 'pressure', 'å‹åŠ›', 'bar', 'psi']
        }
        
        # ä¸ºæ¯ä¸ªç±»åˆ«æ‰¾åˆ°åŒ¹é…çš„åˆ—
        for category, keywords in mapping_rules.items():
            matching_columns = []
            
            for col_info in all_columns:
                col_lower = col_info["column_lower"]
                
                # è®¡ç®—åŒ¹é…åˆ†æ•°
                match_score = 0
                matched_keywords = []
                
                for keyword in keywords:
                    if keyword.lower() in col_lower:
                        match_score += 1
                        matched_keywords.append(keyword)
                
                if match_score > 0:
                    matching_columns.append({
                        "column": col_info["column"],
                        "sheet": col_info["sheet"],
                        "match_score": match_score,
                        "matched_keywords": matched_keywords
                    })
            
            # æŒ‰åŒ¹é…åˆ†æ•°æ’åº
            matching_columns.sort(key=lambda x: x["match_score"], reverse=True)
            mappings[category] = matching_columns[:5]  # ä¿ç•™å‰5ä¸ªæœ€ä½³åŒ¹é…
        
        return mappings
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "total_worksheets": len(self.all_worksheets),
            "total_columns": 0,
            "total_rows": 0,
            "worksheets_with_data": 0,
            "likely_hex_worksheets": [],
            "data_quality_issues": [],
            "extraction_readiness": "unknown"
        }
        
        for sheet_name, df in self.all_worksheets.items():
            if isinstance(df, pd.DataFrame):
                summary["total_columns"] += len(df.columns)
                summary["total_rows"] += len(df)
                if not df.empty:
                    summary["worksheets_with_data"] += 1
        
        # è¯†åˆ«å¯èƒ½çš„çƒ­äº¤æ¢å™¨å·¥ä½œè¡¨
        for sheet_name, info in self.analysis_results.get("data_patterns", {}).get("potential_hex_data", {}).items():
            if info.get("likely_hex_sheet", False):
                summary["likely_hex_worksheets"].append({
                    "sheet": sheet_name,
                    "score": info.get("hex_score", 0),
                    "indicators": info.get("indicators", [])
                })
        
        # è¯„ä¼°æå–å‡†å¤‡åº¦
        if len(summary["likely_hex_worksheets"]) > 0:
            summary["extraction_readiness"] = "good"
        elif summary["worksheets_with_data"] > 0:
            summary["extraction_readiness"] = "needs_analysis"
        else:
            summary["extraction_readiness"] = "poor"
        
        return summary
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        summary = self.analysis_results.get("summary", {})
        
        # åŸºäºå·¥ä½œè¡¨æ•°é‡çš„å»ºè®®
        if summary.get("total_worksheets", 0) > 1:
            recommendations.append("å»ºè®®ä¿®æ”¹ä»£ç æ”¯æŒå¤šå·¥ä½œè¡¨è¯»å–ï¼Œå½“å‰åªè¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨")
        
        # åŸºäºçƒ­äº¤æ¢å™¨æ•°æ®çš„å»ºè®®
        likely_hex_sheets = summary.get("likely_hex_worksheets", [])
        if len(likely_hex_sheets) > 0:
            sheet_names = [sheet["sheet"] for sheet in likely_hex_sheets]
            recommendations.append(f"å‘ç°å¯èƒ½åŒ…å«çƒ­äº¤æ¢å™¨æ•°æ®çš„å·¥ä½œè¡¨: {sheet_names}")
            recommendations.append("å»ºè®®ä¼˜å…ˆä»è¿™äº›å·¥ä½œè¡¨æå–æ•°æ®")
        
        # åŸºäºåˆ—åæ˜ å°„çš„å»ºè®®
        column_analysis = self.analysis_results.get("column_analysis", {})
        suggested_mappings = column_analysis.get("suggested_mappings", {})
        
        for category, mappings in suggested_mappings.items():
            if len(mappings) > 0:
                best_match = mappings[0]
                recommendations.append(f"å»ºè®®å°†åˆ— '{best_match['column']}' æ˜ å°„ä¸º {category}")
        
        # åŸºäºå‘½åçº¦å®šçš„å»ºè®®
        naming_conventions = column_analysis.get("naming_conventions", {})
        if naming_conventions.get("chinese_columns", 0) > 0:
            recommendations.append("å‘ç°ä¸­æ–‡åˆ—åï¼Œå»ºè®®å¢åŠ ä¸­æ–‡å…³é”®è¯åŒ¹é…")
        
        if naming_conventions.get("space_columns", 0) > naming_conventions.get("underscore_columns", 0):
            recommendations.append("åˆ—åå¤šä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œå»ºè®®è°ƒæ•´åŒ¹é…æ¨¡å¼")
        
        return recommendations
    
    def print_analysis_report(self):
        """æ‰“å°åˆ†ææŠ¥å‘Š"""
        if not self.analysis_results:
            print("âŒ å°šæœªæ‰§è¡Œåˆ†æ")
            return
        
        print("\n" + "=" * 80)
        print("ğŸ“Š EXCELæ–‡ä»¶ç»“æ„åˆ†ææŠ¥å‘Š")
        print("=" * 80)
        
        # æ–‡ä»¶ä¿¡æ¯
        file_info = self.analysis_results.get("file_info", {})
        print(f"\nğŸ“ æ–‡ä»¶ä¿¡æ¯:")
        print(f"   æ–‡ä»¶è·¯å¾„: {file_info.get('file_path', 'N/A')}")
        print(f"   æ–‡ä»¶å¤§å°: {file_info.get('file_size_mb', 'N/A')} MB")
        print(f"   æœ€åä¿®æ”¹: {file_info.get('last_modified', 'N/A')}")
        
        # æ‘˜è¦ä¿¡æ¯
        summary = self.analysis_results.get("summary", {})
        print(f"\nğŸ“ˆ æ‘˜è¦ä¿¡æ¯:")
        print(f"   å·¥ä½œè¡¨æ€»æ•°: {summary.get('total_worksheets', 0)}")
        print(f"   æ€»åˆ—æ•°: {summary.get('total_columns', 0)}")
        print(f"   æ€»è¡Œæ•°: {summary.get('total_rows', 0)}")
        print(f"   æœ‰æ•°æ®çš„å·¥ä½œè¡¨: {summary.get('worksheets_with_data', 0)}")
        print(f"   æå–å‡†å¤‡åº¦: {summary.get('extraction_readiness', 'unknown')}")
        
        # å·¥ä½œè¡¨è¯¦æƒ…
        worksheets = self.analysis_results.get("worksheets", {})
        print(f"\nğŸ“‹ å·¥ä½œè¡¨è¯¦æƒ…:")
        for sheet_name, sheet_info in worksheets.items():
            if "error" in sheet_info:
                print(f"   âŒ {sheet_name}: {sheet_info['error']}")
            else:
                print(f"   âœ… {sheet_name}: {sheet_info['row_count']} è¡Œ Ã— {sheet_info['column_count']} åˆ—")
                print(f"      åˆ—å: {sheet_info['columns'][:5]}{'...' if len(sheet_info['columns']) > 5 else ''}")
        
        # çƒ­äº¤æ¢å™¨æ•°æ®è¯†åˆ«
        likely_hex_sheets = summary.get("likely_hex_worksheets", [])
        if likely_hex_sheets:
            print(f"\nğŸ”¥ å¯èƒ½çš„çƒ­äº¤æ¢å™¨æ•°æ®å·¥ä½œè¡¨:")
            for sheet_info in likely_hex_sheets:
                print(f"   ğŸ¯ {sheet_info['sheet']} (è¯„åˆ†: {sheet_info['score']})")
                for indicator in sheet_info['indicators']:
                    print(f"      â€¢ {indicator}")
        
        # ä¿®å¤å»ºè®®
        recommendations = self.analysis_results.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("\n" + "=" * 80)
    
    def save_analysis_to_json(self, output_file: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        if not output_file:
            output_file = f"excel_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj
            
            converted_results = convert_numpy_types(self.analysis_results)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(converted_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
            return ""


def main():
    """ä¸»å‡½æ•°"""
    excel_file = "BFG-CO2H-HEX.xlsx"
    
    if not os.path.exists(excel_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ExcelStructureAnalyzer(excel_file)
    
    # æ‰§è¡Œå®Œæ•´åˆ†æ
    results = analyzer.analyze_complete_structure()
    
    # æ‰“å°æŠ¥å‘Š
    analyzer.print_analysis_report()
    
    # ä¿å­˜ç»“æœ
    analyzer.save_analysis_to_json()
    
    print(f"\nğŸ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()