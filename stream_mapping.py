#!/usr/bin/env python3
"""
æµè‚¡åç§°æ˜ å°„å·¥å…·

ç”¨äºå°†æ•°æ®åº“ä¸­çš„æµè‚¡åç§°ä¸Aspen Plusä¸­çš„æµè‚¡åç§°è¿›è¡ŒåŒ¹é…
"""

import sqlite3
import sys
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import re
from difflib import SequenceMatcher
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class StreamMapping:
    """æµè‚¡æ˜ å°„æ•°æ®ç»“æ„"""
    database_name: str
    aspen_name: str
    similarity_score: float
    mapping_reason: str
    confidence: float

class StreamNameMatcher:
    """æµè‚¡åç§°åŒ¹é…å™¨"""
    
    def __init__(self, db_path: str = "aspen_data.db"):
        self.db_path = db_path
        self.database_streams = []
        self.aspen_streams = []
        self.mappings = []
        
    def load_database_streams(self) -> List[str]:
        """ä»æ•°æ®åº“åŠ è½½æµè‚¡åç§°"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            logger.info(f"æ•°æ®åº“ä¸­çš„è¡¨: {tables}")
            
            # å°è¯•ä¸åŒçš„è¡¨åå’Œåˆ—åç»„åˆ
            possible_queries = [
                ("streams", "SELECT stream_name FROM streams ORDER BY stream_name"),
                ("aspen_streams", "SELECT stream_name FROM aspen_streams ORDER BY stream_name"),
                ("aspen_streams", "SELECT name FROM aspen_streams ORDER BY name"),
                ("aspen_streams", "SELECT stream_id FROM aspen_streams ORDER BY stream_id"),
                ("aspen_streams", "SELECT * FROM aspen_streams LIMIT 1")  # æŸ¥çœ‹ç»“æ„
            ]
            
            streams = []
            for table_name, query in possible_queries:
                if table_name in tables:
                    try:
                        cursor.execute(query)
                        results = cursor.fetchall()
                        
                        if "SELECT *" in query:
                            # æŸ¥çœ‹è¡¨ç»“æ„
                            if results:
                                logger.info(f"aspen_streamsè¡¨ç¬¬ä¸€è¡Œæ•°æ®: {results[0]}")
                                # è·å–åˆ—å
                                cursor.execute("PRAGMA table_info(aspen_streams)")
                                columns = cursor.fetchall()
                                column_names = [col[1] for col in columns]
                                logger.info(f"åˆ—å: {column_names}")
                                
                                # å°è¯•ç”¨ç¬¬ä¸€ä¸ªå¯èƒ½çš„åç§°åˆ—
                                name_columns = [col for col in column_names if 'name' in col.lower() or 'id' in col.lower()]
                                if name_columns:
                                    name_col = name_columns[0]
                                    cursor.execute(f"SELECT {name_col} FROM aspen_streams ORDER BY {name_col}")
                                    streams = [row[0] for row in cursor.fetchall()]
                                    logger.info(f"ä½¿ç”¨åˆ— {name_col} æ‰¾åˆ° {len(streams)} ä¸ªæµè‚¡")
                                    break
                        else:
                            streams = [row[0] for row in results]
                            logger.info(f"æŸ¥è¯¢æˆåŠŸï¼Œæ‰¾åˆ° {len(streams)} ä¸ªæµè‚¡")
                            break
                            
                    except Exception as e:
                        logger.debug(f"æŸ¥è¯¢å¤±è´¥ {query}: {e}")
                        continue
            
            conn.close()
            self.database_streams = streams
            return streams
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®åº“æµè‚¡æ—¶å‡ºé”™: {e}")
            return []
    
    def load_aspen_streams(self) -> List[str]:
        """ä»æœ€è¿‘çš„Aspenæå–ä¸­åŠ è½½æµè‚¡åç§°"""
        # è¿™äº›æ˜¯æˆ‘ä»¬åœ¨æµ‹è¯•ä¸­çœ‹åˆ°çš„Aspenæµè‚¡åç§°
        aspen_streams = [
            'AF-COM', 'AIR', 'BFG', 'CS1', 'FLUEGAS1', 'GASOUT1', 'H2IN',
            'LIGHTEND', 'MEOH1', 'MEOH2', 'MEOH3', 'MEOH4', 'MEOH5', 'MEOH6',
            'MEOH7', 'P1', 'PUR2', 'PUR3', 'PUR4', 'REF4', 'REF6', 'S1', 'SS2', 'SS3'
        ]
        
        self.aspen_streams = aspen_streams
        logger.info(f"åŠ è½½äº† {len(aspen_streams)} ä¸ªAspenæµè‚¡")
        return aspen_streams
    
    def calculate_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()
    
    def find_keyword_matches(self, db_name: str, aspen_name: str) -> Tuple[bool, str]:
        """åŸºäºå…³é”®è¯æŸ¥æ‰¾åŒ¹é…"""
        db_lower = db_name.lower()
        aspen_lower = aspen_name.lower()
        
        # å®šä¹‰å…³é”®è¯æ˜ å°„è§„åˆ™
        keyword_mappings = {
            # é«˜ç‚‰ç…¤æ°”ç›¸å…³
            'bfg': ['bfg', 'blast', 'furnace'],
            'feed': ['feed', 'input', 'in'],
            'product': ['product', 'output', 'out'],
            'methanol': ['meoh', 'methanol', 'ch3oh'],
            'water': ['water', 'h2o', 'cooling'],
            'steam': ['steam', 'vapor', 'hp', 'lp', 'mp'],
            'air': ['air'],
            'hydrogen': ['h2', 'hydrogen'],
            'purge': ['pur', 'purge'],
            'recycle': ['recycle', 'rec'],
            'flash': ['flash', 'separator'],
            'condenser': ['condenser', 'cond'],
            'makeup': ['makeup', 'make-up'],
            'light': ['light', 'lightend'],
            'reference': ['ref', 'reference'],
            'gas': ['gas', 'gasout', 'flue'],
            'liquid': ['liquid', 'liq'],
            'co2': ['co2', 'carbon', 'dioxide'],
            'reactor': ['rxn', 'reactor', 'reaction'],
            'distillation': ['t-', 'tower', 'distil'],
            'ss': ['ss', 'stainless']
        }
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        for category, keywords in keyword_mappings.items():
            db_match = any(kw in db_lower for kw in keywords)
            aspen_match = any(kw in aspen_lower for kw in keywords)
            
            if db_match and aspen_match:
                return True, f"å…³é”®è¯åŒ¹é…: {category}"
        
        # ç‰¹æ®Šè§„åˆ™åŒ¹é…
        special_rules = [
            # é«˜ç‚‰ç…¤æ°”
            (('bfg' in db_lower and 'feed' in db_lower), ('bfg' in aspen_lower), "BFGåŸæ–™åŒ¹é…"),
            # äºŒæ°§åŒ–ç¢³
            (('co2' in db_lower and 'feed' in db_lower), ('co2' in aspen_lower or 'af-com' in aspen_lower), "CO2åŸæ–™åŒ¹é…"),
            # ç”²é†‡äº§å“
            (('methanol' in db_lower and 'product' in db_lower), ('meoh' in aspen_lower), "ç”²é†‡äº§å“åŒ¹é…"),
            # æ°´äº§å“
            (('water' in db_lower and 'product' in db_lower), ('h2o' in aspen_lower or 'water' in aspen_lower), "æ°´äº§å“åŒ¹é…"),
            # å†·å´æ°´
            (('cooling' in db_lower and 'water' in db_lower), ('cooling' in aspen_lower or 'water' in aspen_lower), "å†·å´æ°´åŒ¹é…"),
            # è’¸æ±½
            (('steam' in db_lower), ('steam' in aspen_lower or any(x in aspen_lower for x in ['hp', 'lp', 'mp'])), "è’¸æ±½åŒ¹é…"),
            # æ°¢æ°”
            (('h2' in db_lower and 'makeup' in db_lower), ('h2' in aspen_lower), "æ°¢æ°”è¡¥å……åŒ¹é…"),
            # å¹æ‰«æ°”
            (('purge' in db_lower), ('pur' in aspen_lower), "å¹æ‰«æ°”åŒ¹é…"),
            # å¾ªç¯æ°”
            (('recycle' in db_lower), ('rec' in aspen_lower or 'ref' in aspen_lower), "å¾ªç¯æ°”åŒ¹é…"),
            # ååº”å™¨
            (('rxn' in db_lower or 'reactor' in db_lower), ('rxn' in aspen_lower or 'reactor' in aspen_lower), "ååº”å™¨åŒ¹é…"),
            # åˆ†ç¦»å™¨/é—ªè’¸
            (('flash' in db_lower), ('flash' in aspen_lower or 'lightend' in aspen_lower), "åˆ†ç¦»å™¨åŒ¹é…"),
            # å†·å‡å™¨
            (('condenser' in db_lower), ('condenser' in aspen_lower or 'cs' in aspen_lower), "å†·å‡å™¨åŒ¹é…"),
        ]
        
        for db_condition, aspen_condition, reason in special_rules:
            if db_condition and aspen_condition:
                return True, reason
        
        return False, "æ— å…³é”®è¯åŒ¹é…"
    
    def create_stream_mappings(self) -> List[StreamMapping]:
        """åˆ›å»ºæµè‚¡æ˜ å°„"""
        mappings = []
        
        if not self.database_streams or not self.aspen_streams:
            logger.warning("æµè‚¡æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºæ˜ å°„")
            return mappings
        
        # ä¸ºæ¯ä¸ªæ•°æ®åº“æµè‚¡æ‰¾åˆ°æœ€ä½³åŒ¹é…
        for db_stream in self.database_streams:
            best_match = None
            best_score = 0.0
            best_reason = ""
            
            for aspen_stream in self.aspen_streams:
                # è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                similarity = self.calculate_similarity(db_stream, aspen_stream)
                
                # æ£€æŸ¥å…³é”®è¯åŒ¹é…
                keyword_match, keyword_reason = self.find_keyword_matches(db_stream, aspen_stream)
                
                # ç»¼åˆè¯„åˆ†
                score = similarity
                reason = f"å­—ç¬¦ä¸²ç›¸ä¼¼åº¦: {similarity:.2f}"
                
                if keyword_match:
                    score += 0.3  # å…³é”®è¯åŒ¹é…åŠ åˆ†
                    reason += f", {keyword_reason}"
                
                # ç²¾ç¡®åŒ¹é…åŠ åˆ†
                if db_stream.lower() == aspen_stream.lower():
                    score = 1.0
                    reason = "ç²¾ç¡®åŒ¹é…"
                
                if score > best_score:
                    best_score = score
                    best_match = aspen_stream
                    best_reason = reason
            
            # åªä¿ç•™ç½®ä¿¡åº¦è¾ƒé«˜çš„åŒ¹é…
            if best_score > 0.3:  # é˜ˆå€¼
                confidence = min(best_score, 1.0)
                mapping = StreamMapping(
                    database_name=db_stream,
                    aspen_name=best_match,
                    similarity_score=best_score,
                    mapping_reason=best_reason,
                    confidence=confidence
                )
                mappings.append(mapping)
        
        self.mappings = mappings
        return mappings
    
    def print_mappings(self):
        """æ‰“å°æ˜ å°„ç»“æœ"""
        if not self.mappings:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æµè‚¡")
            return
        
        print("\n" + "="*80)
        print("ğŸ”— æµè‚¡åç§°æ˜ å°„ç»“æœ")
        print("="*80)
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_mappings = sorted(self.mappings, key=lambda x: x.confidence, reverse=True)
        
        high_confidence = [m for m in sorted_mappings if m.confidence >= 0.8]
        medium_confidence = [m for m in sorted_mappings if 0.5 <= m.confidence < 0.8]
        low_confidence = [m for m in sorted_mappings if m.confidence < 0.5]
        
        if high_confidence:
            print(f"\nğŸŸ¢ é«˜ç½®ä¿¡åº¦æ˜ å°„ ({len(high_confidence)} ä¸ª):")
            print("-" * 60)
            for mapping in high_confidence:
                print(f"  {mapping.database_name:20} â†’ {mapping.aspen_name:15} "
                     f"(ç½®ä¿¡åº¦: {mapping.confidence:.2f})")
                print(f"    ğŸ“‹ {mapping.mapping_reason}")
        
        if medium_confidence:
            print(f"\nğŸŸ¡ ä¸­ç­‰ç½®ä¿¡åº¦æ˜ å°„ ({len(medium_confidence)} ä¸ª):")
            print("-" * 60)
            for mapping in medium_confidence:
                print(f"  {mapping.database_name:20} â†’ {mapping.aspen_name:15} "
                     f"(ç½®ä¿¡åº¦: {mapping.confidence:.2f})")
                print(f"    ğŸ“‹ {mapping.mapping_reason}")
        
        if low_confidence:
            print(f"\nğŸ”´ ä½ç½®ä¿¡åº¦æ˜ å°„ ({len(low_confidence)} ä¸ª):")
            print("-" * 60)
            for mapping in low_confidence:
                print(f"  {mapping.database_name:20} â†’ {mapping.aspen_name:15} "
                     f"(ç½®ä¿¡åº¦: {mapping.confidence:.2f})")
                print(f"    ğŸ“‹ {mapping.mapping_reason}")
        
        print(f"\nğŸ“Š æ˜ å°„ç»Ÿè®¡:")
        print(f"  â€¢ æ•°æ®åº“æµè‚¡: {len(self.database_streams)}")
        print(f"  â€¢ Aspenæµè‚¡: {len(self.aspen_streams)}")
        print(f"  â€¢ æˆåŠŸæ˜ å°„: {len(self.mappings)}")
        print(f"  â€¢ æ˜ å°„ç‡: {len(self.mappings)/len(self.database_streams)*100:.1f}%")
    
    def save_mappings_to_database(self) -> bool:
        """å°†æ˜ å°„ç»“æœä¿å­˜åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºæ˜ å°„è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS stream_mappings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    database_name TEXT NOT NULL,
                    aspen_name TEXT NOT NULL,
                    similarity_score REAL,
                    mapping_reason TEXT,
                    confidence REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(database_name, aspen_name)
                )
            """)
            
            # æ¸…é™¤æ—§æ˜ å°„
            cursor.execute("DELETE FROM stream_mappings")
            
            # æ’å…¥æ–°æ˜ å°„
            for mapping in self.mappings:
                cursor.execute("""
                    INSERT INTO stream_mappings 
                    (database_name, aspen_name, similarity_score, mapping_reason, confidence)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    mapping.database_name,
                    mapping.aspen_name,
                    mapping.similarity_score,
                    mapping.mapping_reason,
                    mapping.confidence
                ))
            
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(self.mappings)} ä¸ªæ˜ å°„åˆ°æ•°æ®åº“")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ˜ å°„åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            return False
    
    def get_mapping_dict(self) -> Dict[str, str]:
        """è·å–æ˜ å°„å­—å…¸ (æ•°æ®åº“å -> Aspenå)"""
        return {m.database_name: m.aspen_name for m in self.mappings}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æµè‚¡åç§°æ˜ å°„å·¥å…·")
    print("="*50)
    
    # åˆ›å»ºåŒ¹é…å™¨
    matcher = StreamNameMatcher()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“¥ åŠ è½½æµè‚¡æ•°æ®...")
    db_streams = matcher.load_database_streams()
    aspen_streams = matcher.load_aspen_streams()
    
    if not db_streams:
        print("âŒ æ— æ³•ä»æ•°æ®åº“åŠ è½½æµè‚¡æ•°æ®")
        return
    
    print(f"ğŸ“‹ æ•°æ®åº“æµè‚¡ ({len(db_streams)} ä¸ª):")
    for i, stream in enumerate(db_streams, 1):
        print(f"  {i:2d}. {stream}")
    
    print(f"\nğŸ“‹ Aspenæµè‚¡ ({len(aspen_streams)} ä¸ª):")
    for i, stream in enumerate(aspen_streams, 1):
        print(f"  {i:2d}. {stream}")
    
    # åˆ›å»ºæ˜ å°„
    print("\nğŸ”„ åˆ›å»ºæµè‚¡æ˜ å°„...")
    mappings = matcher.create_stream_mappings()
    
    # æ˜¾ç¤ºç»“æœ
    matcher.print_mappings()
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    print("\nğŸ’¾ ä¿å­˜æ˜ å°„åˆ°æ•°æ®åº“...")
    if matcher.save_mappings_to_database():
        print("âœ… æ˜ å°„ä¿å­˜æˆåŠŸ")
    else:
        print("âŒ æ˜ å°„ä¿å­˜å¤±è´¥")

if __name__ == "__main__":
    main()
